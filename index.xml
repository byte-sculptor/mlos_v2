<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MLOS</title>
    <link>https://microsoft.github.io/MLOS/</link>
    <description>Recent content on MLOS</description>
    <generator>Hugo -- gohugo.io</generator>
    
	<atom:link href="https://microsoft.github.io/MLOS/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/CODE_OF_CONDUCT/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/CODE_OF_CONDUCT/</guid>
      <description>Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct.
Resources:
 Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/CONTRIBUTING/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/CONTRIBUTING/</guid>
      <description>Contributing to MLOS This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.
When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/01-Prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/01-Prerequisites/</guid>
      <description>Prerequisites for building and using MLOS These are one-time setup instructions that should be executed prior to following the build instructions in 02-Build.md
Contents  Prerequisites for building and using MLOS  Contents Clone the repository Python quickstart Linux  Linux Distribution Requirements Option 1: Linux Docker Build Env  Install Docker Build the Docker Image  Pull the upstream docker image Local docker image build     Option 2: Manual Build Tools Install Install Python on Linux  Option 1: Docker Python Install Option 2: Using Conda     Windows  Step 1: Install Python Step 2: Install Docker on Windows Step 3: Install Windows Build Tools Step 4: Build the Docker image      MLOS currently supports 64-bit Intel/AMD platforms, though ARM64 support is under development.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/02-Build/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/02-Build/</guid>
      <description>Build Instructions for MLOS Prerequisites See 01-Prerequisites.md for initial build tools setup instructions.
There are different instructions according to the environment setup you chose.
Contents  Build Instructions for MLOS  Prerequisites Contents Docker  Create a new container instance  Using the upstream container image Using the locally built image   Other useful docker commands Start an existing container instance Get a new shell in a running container instance   Linux  CLI: make VSCode   Windows  CLI: msbuild Building with Visual Studio      Docker If you chose to use the Docker build environment and have already built or pulled a container image using the instructions in 01-Prerequisites.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/03-ExampleUsage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/03-ExampleUsage/</guid>
      <description>Examples of using MLOS to optimize a system TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/04-Test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/04-Test/</guid>
      <description>Test Instructions for MLOS Contents  Test Instructions for MLOS  Contents Linux Tests  Run C# Tests on Linux Run C++ Tests on Linux Run Python Tests on Linux   Windows  Run C#/C++ Tests on Windows Run Python Tests on Windows      Linux Tests To build and test all of the MLOS code at or below the current folder, regardless of language, run:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/05-Debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/05-Debug/</guid>
      <description>TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/CodingStandard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/CodingStandard/</guid>
      <description>MLOS Coding Standards MLOS uses and supports multiple languages. Here we document the coding styles and standards we attempt to adhere to and the tools we use to achieve that.
C++ For C++ we mostly try to follow the Google C++ style guidelines, with a few modifications.
Currently we rely on uncrustify to help enforce these rules (plus a little bit of human review).
See build/uncrustify/README.md for additional information.
Though we attempt to make it somewhat readable, we exclude code generated by MLOS from these strict style checks.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/Glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/Glossary/</guid>
      <description>MLOS Terms Glossary TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/MlosArchitecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/MlosArchitecture/</guid>
      <description>MLOS Architecture This document provides a brief overview of the MLOS architecture for supporting Machine Learning Optimized Systems.
 MLOS Architecture  High Level Description  Principles Workflows   Architecture Diagram  Main components Shared Memory Regions Target Process  Mlos.Core Shared Channel   Mlos.Agent  Mlos.NetCore Settings registry assemblies Grpc Server Experiment management     Implementation details    High Level Description At a high level, MLOS provides infrastructure to support instance-specific tuning systems software (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/RepoOrganization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/RepoOrganization/</guid>
      <description>Repo Organization Some notes on the directory layout organization in this repo.
  There are build files (e.g. dirs.proj for msbuild or dotnet build, or Makefiles for make) in most directories to allow easy recursive building of that subtree you happen to be in.
 Note: we provide Makefile wrappers in most directories to simply help invoke cmake and the Makefiles it generates
   build/ contains configuration related to building MLOS components</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/Troubleshooting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/Troubleshooting/</guid>
      <description>Troubleshooting Tips Here are some common tips for troubleshooting various issues.
Contents  Troubleshooting Tips  Contents Editor Integrations  VSCode in WSL  &amp;ldquo;Missing .Net SDK&amp;rdquo; message when executing code . in WSL        Editor Integrations VSCode in WSL &amp;ldquo;Missing .Net SDK&amp;rdquo; message when executing code . in WSL The Omnisharp plugin for VSCode may have trouble finding the dotnet setup locally for the MLOS repo in tools/, even if you source the scripts/init.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/BayesianOptimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/BayesianOptimization/</guid>
      <description>Download BayesianOptimization.ipynb notebook from IPython.core.display import display, HTML import warnings display(HTML(&amp;#34;&amp;lt;style&amp;gt;.container { width:100% !important; }&amp;lt;/style&amp;gt;&amp;#34;)) warnings.simplefilter(&amp;#34;ignore&amp;#34;) &amp;lt;IPython.core.display.HTML object&amp;gt;  import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.stats import t Bayesian Optimization This notebook demonstrates the basic principles of Bayesian Optimization (BO) and how to use MLOS to perform BO.
Motivation In software performance engineering, the impact different (input) parameters (e.g. buffer size, worker thread count, etc.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/LevelDbTuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/LevelDbTuning/</guid>
      <description>Download LevelDbTuning.ipynb notebook LevelDB parameter tuning using MLOS What is Level DB LevelDB is a key value store built using Log Structured Merge Trees (LSMs) Wiki. LevelDB supports read, write, delete and range query (sorted iteration) operations.
Typical to any database system, LevelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we&amp;rsquo;ll briefly describe the working of LevelDB.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/SmartCacheCPP/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/SmartCacheCPP/</guid>
      <description>Download SmartCacheCPP.ipynb notebook Connecting MLOS to a C++ application This notebook walks through connecting MLOS to a C++ application within a docker container. We will start a docker container, and run an MLOS Agent within it. The MLOS Agent will start the actual application, and communicate with it via a shared memory channel. In this example, the MLOS Agent controls the execution of the workloads on the application, and we will later connect to the agent to optimize the configuration of our application.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/SmartCacheOptimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/SmartCacheOptimization/</guid>
      <description>Download SmartCacheOptimization.ipynb notebook Optimizing Smart Cache with Bayesian Optimization The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.
We&amp;rsquo;re using a sequential model-based optimization approach, that consists of the following loop:
 Get suggested config from optimizer, Apply suggested config to SmartCache, Execute a fixed workload, Collect the metrics from SmartCache, Register an observation with the optimizer.  # import the required classes and tools import grpc import pandas as pd import logging from mlos.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/SECURITY/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/SECURITY/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedChannel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedChannel/</guid>
      <description>MLOS Shared Channel This document describes the implementation details of the mechanism (a shared memory communication channel) used for a target system to communicate with an external agent for tuning it.
For additional context, please see the MlosArchitecture.md documentation.
See source/Mlos.Core to browse the code.
Contents  MLOS Shared Channel  Contents Shared Channel  Principles Circular buffer algorithm  Writer Reader Writer continued Reader continue Cyclic buffer handling   Scaling out readers   Shared channel implementation  Diagram Policies Notes      Shared Channel A shared channel is a one-directional communication channel based on a single shared memory block (i.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedMemoryManagement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedMemoryManagement/</guid>
      <description>Shared Memory Management Bootstrap sequence Overview Target process, process which hold tunable components.
MLOS Agent - the agent process, responsible for collecting the telemetry and communicating with the optimizer.
OS implementation differences. MLOS is handling shared memory differently on Windows and Linux.
Linux is creating anonymous shared memory, and it is passing a file descriptor between the processes using Unix domain socket.
The target process is responsible for creating shared memory regions and channel synchronization primitives.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.NetCore/Doc/SharedChannelScaleout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.NetCore/Doc/SharedChannelScaleout/</guid>
      <description>Shared Channel Scaleout Document contains results of the improvement in the shared communication channel. We specifically address lack of the scalability issue. The channel does not scale well with increasing number of readers and writers operating on the same channel instance.
Benchmark The benchmark is implemented in Mlos.NetCore.Benchmark project
Benchmark:
 $(MLOSROOT)\MLOS\out\obj\Source\Mlos.NetCore.Benchmark\obj\amd64\Mlos.NetCore.Benchmark.exe -i &amp;ndash;filter SharedChannelReaderScaleOutBenchmarks
 Results Hyper-threading is disabled.
 Intel E5-2670 v3 @ 2.30 GHz:    ReaderCount Mean[B] Mean[I] Error[B] Error[I] StdDev[B] StdDev[I] Allocated     1 830.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerArchitecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerArchitecture/</guid>
      <description>Bayesian Optimizer Architecture Components  Surrogate Models Utility Functions Numeric Optimizers Experiment Designer Bayesian Optimizer  Other Classes  Hypergrids Optimization Problems  Hypergrids Hypergrids are used to describe multidimensional spaces comprized of Continuous, Discrete, Ordinal, or Categorical dimensions.
All optimizers we have reviewed to date (Hypermapper, SMAC, bayesopt, and scikit-learn) optimizers implement their own notion of a search space.
Hypergrids are meant to provide a superset of their functionalities. They further allow us to express hierarchical search spaces, where some parameters only become meaningful if some other parameter (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerV1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerV1/</guid>
      <description>Bayesian Optimizer V1 The goal of this document is to describe the architecture and inner workings of a Bayesian Optimizer.
Components The following components will be necessary:
 Experiment Observation Storage (Table in SQL Server) Bayesian Optimizer Surrogate Models  Architecture TODO: describe how we will use the technologies:
 Docker (K8S?) SQL Server Python ML.Net SQL DB RPC vs. gRPC  Obviously an experiment and the corresponding bayesian optimizer have to be compatible so that the observations generated by the experiment are within the optimizers observation space.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/OptimizerMonitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/OptimizerMonitoring/</guid>
      <description>Optimizer Monitoring Motivation The goal of this document is to outline the process of monitoring the optimizers, enumerate the metrics we wish to collect and the tools required to do so.
What we wish to monitor For each optimizer we should be able to:
 View it&amp;rsquo;s current configuration (done). View all the data that it was trained on. View the state of the surrogate models:  Have they been fitted?</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/SimpleImportGraph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/SimpleImportGraph/</guid>
      <description>digraph dfd2 {node[shape=record]&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;DecisionTreeRegressionModel.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizerInterface.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;OptimizerInterface.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;ExperimentDesigner.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;Prediction.py&amp;quot;&amp;quot;DecisionTreeRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;ConfidenceBoundUtilityFunction.py&amp;quot;&amp;quot;RandomSearchOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;DecisionTreeRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;Prediction.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;RandomSearchOptimizer.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot;}</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Examples/SmartCache/OverviewOfCachingStrategies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Examples/SmartCache/OverviewOfCachingStrategies/</guid>
      <description>Overview of Caching Strategies and Algorithms Goal The objective of this document is to describe various approaches to caching. The intent is to then implement a number of paramiterized caching algorithms and allow MLOS to select between them on a per-workload basis.
Links A loose list of sources consulted in this survey:
 https://medium.com/datadriveninvestor/all-things-caching-use-cases-benefits-strategies-choosing-a-caching-technology-exploring-fa6c1f2e93aa https://en.wikipedia.org/wiki/Cache_replacement_policies  Potential Objectives  average retrieval time/cost/latency (or other statistics: percentiles, CI&amp;rsquo;s etc) hit ratio and miss ratio cache hit latency metrics (in case of a hit) data staleness metrics (distribution of time since last usage among all cache entries)  Plausible Implementations/Approaches  FIFO - a queue and a hash-map would work.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Spaces/HypergridAdapters/AboutHypergridAdapters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Spaces/HypergridAdapters/AboutHypergridAdapters/</guid>
      <description>Hypergrid Adapters Motivation Categorical to numeric projections The goal of adapters is to make a broader class of hypergrids compatible with any surrogate model. The chief problem in absence of adapters is that some models (RERF, DecisionTreeRegressionModel) can only operate on numeric datatypes, but the configuration for many components includes strings, and booleans as well. A suitable adapter will map such categorical dimensions into numeric ones and allow transparent projection of observations and suggestions between the components and the regression models.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Streaming/Doc/AggregateStreaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Streaming/Doc/AggregateStreaming/</guid>
      <description>Aggregates on the telemetry streams Intro Processing models     Single return value Multiple return values     Pull/Synchronous/Interactive T IEnumerable&amp;lt;T&amp;gt;   Push/Asynchronous/Reactive Task&amp;lt;T&amp;gt; IObservable&amp;lt;T&amp;gt;    MLOS Telemetry channel Processing events   why not async:
 introduces dedicated processing tasks additional latency introduced by AsyncQueue    why we need a push model
  Linq operators on observable streams Merging streams operator Collecting the results  in progress  Performance improvements  in progress  </description>
    </item>
    
  </channel>
</rss>