{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Add ability to dump trace to file.\n",
    "2. Trace client-server interaction.\n",
    "3. Add tracking convergence state to this notebook.\n",
    "4. Move tracking convergence state to optimizer.\n",
    "5. Separate ML-config from admin-config. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this notebook is to demonstrate the use of the Optimizer Evaluation Tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer evaluation purpose\n",
    "\n",
    "The goal of optimizer evaluation is to learn how to best match an optimizer configuration to an optimization problem.\n",
    "\n",
    "## Optimizer evaluation strategy\n",
    "\n",
    "Broadly, the optimizer evaluation strategy hinges on characterizing the optimizers' performance on a variety of optimization problems, under a variety of configurations. This should allow us to:\n",
    "* find the strengths and limitations of the various optimizer configurations,\n",
    "* discard the dominated configurations,\n",
    "* ultimately match optimizer configurations with the problem.\n",
    "\n",
    "Note: No absolute scale of optimzier performance seems to have garnered consensus in the community. Thus, in this exercise we will compare the optimizers with each other. We can also compare them to some published results, though it might be good to replicate them first.\n",
    "\n",
    "A common way to compare multiple optimizers is to rank them for each problem given a particular budget (usually in terms of number of evaluations, though if we assume that the function is cheap to compute, optimizer resource consumption might be more relevant). Then we can report average rank across problems to compare the optimizers. Or we can do a matrix for each benchmark problem which optimizer dominates which.\n",
    "\n",
    "\n",
    "### Optimizer performance characteristics\n",
    "\n",
    "We aim to evaluate the following aspects of the optimizers' performance:\n",
    "* convergence - is the optimizer finding the optima, More formally, can the optimizer get within epsilon of the known optimum at all?\n",
    "* rate of convergence - how quickly is the optimizer finding the optima?\n",
    "* trajectory of convergence - is the optimizer improving rapidly at first, and then reaching a plateau, is it climbing steadily, or does it stagnate for a long time, before finally shooting up?\n",
    "* surrogate model goodness of fit - how well do the models fit the training data, validation data (out of bag samples), test data (observations gathered after the model was fit), random test data (random observations gathered after the model was fit). \n",
    "* computational cost - for now we can use the Tracer to capture runtime information, down the road we should monitor CPU, and memory utilization too.\n",
    "\n",
    "### What to measure\n",
    "\n",
    "* Convergence of the entire optimizer on synthetic and real data.\n",
    "* Models' goodness of fit on synthetic and real data, in both on both guided and unguided observations.\n",
    "* Utility function optimizers' performance on synthetic data.\n",
    "\n",
    "We shall measure and plot all of the above metrics as a function of the number of observations that the optimzier has been fit on.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Definition of optimum\n",
    "Various definitions of optimum are possible, and we have built consensus that the user should choose a definition suitable for their purpose. Here are some options:\n",
    "1. Best observation - simplest, but can be deceptive in presence of noise. \n",
    "2. Observations with the highest:\n",
    "    1. predicted mean performance\n",
    "    2. upper confidence bound on performance\n",
    "    3. lower confidence bound on performance\n",
    "3. Speculative optima - return configurations predicted by the surrogate model but not necessarily tested:\n",
    "    1. maximum predicted mean\n",
    "    2. maximum predicted upper confidence bound\n",
    "    3. maximum predicted lower confidence bound\n",
    "        \n",
    "Once we graduate to multi-objective optimization, we will need to build pareto frontiers from the above.\n",
    "<hr>\n",
    "\n",
    "Note, that surrogate models' goodness of fit becomes very important for all but the first definition of optimum.\n",
    "\n",
    "### Selecting the optimizer configuration\n",
    "\n",
    "The ability of an optimizer to converge on an optimum is our fundamental requirement. From between the optimizers that can converge we can break ties using secondary criteria: rate of convergence, goodness of fit, computational complexity of the optimizer.\n",
    "\n",
    "### Troubleshooting the optimizers\n",
    "\n",
    "For the optimizers that do not converge, this framework should illuminate their modes of failure:\n",
    "* Is the model not fitting the data well?\n",
    "* Is the model fitting the data, but the exposed parameters don't affect performance, or is there too much noise?\n",
    "* If the model has strong goodness of fit, and performance is sensitive to the parameters, is the utility function optimizer up to the job? \n",
    "* Are we using the right utility function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Model Evaluation\n",
    "\n",
    "1. Perform model GoF evaluation on some functions out of the factory:\n",
    "    * <s>Define some more configurations for the objective functions.</s>\n",
    "    * <s>Create the objective function</s>\n",
    "    * <s>Create the model</s>\n",
    "    * <s>Feed the data to the model (one observationa at a time, then in batches)</s>\n",
    "    * <s>Plot the GoF metrics as a function of number of iterations (repeat each experiment a few times).</s>\n",
    "    * Define some more configurations for the surrogate model (num trees, refit frequency, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionFactory import ObjectiveFunctionFactory\n",
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionConfigStore import objective_function_config_store\n",
    "#objective_function_config = objective_function_config_store.get_config_by_name('noisy_polynomial_objective')\n",
    "objective_function_config = objective_function_config_store.default\n",
    "objective_function = ObjectiveFunctionFactory.create_objective_function(objective_function_config)\n",
    "\n",
    "# Let's make sure this thing works\n",
    "#\n",
    "random_params_df = objective_function.parameter_space.random_dataframe(num_samples=100)\n",
    "values_df = objective_function.evaluate_dataframe(random_params_df)\n",
    "combined_df = pd.concat([random_params_df, values_df], axis=1)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import mlos.global_values as global_values\n",
    "from mlos.Grpc.OptimizerMicroserviceServer import OptimizerMicroserviceServer\n",
    "from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store\n",
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Tracer import Tracer\n",
    "\n",
    "global_values.declare_singletons()\n",
    "global_values.tracer = Tracer(actor_id=\"OptimizerEvaluationTools\", thread_id=0)\n",
    "\n",
    "# Let's stand up the Optimizer Microservice\n",
    "#\n",
    "server = OptimizerMicroserviceServer(port=50051, num_threads=10)\n",
    "server.start()\n",
    "optimizer_factory = BayesianOptimizerFactory(grpc_channel=grpc.insecure_channel('localhost:50051'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store\n",
    "\n",
    "optimizer_config = bayesian_optimizer_config_store.default\n",
    "optimizer_config.experiment_designer_config.glow_worm_swarm_optimizer_config.num_iterations = 10\n",
    "optimizer_config.min_samples_required_for_guided_design_of_experiments = 10\n",
    "\n",
    "print(optimizer_config.to_json(indent=2))\n",
    "\n",
    "\n",
    "# Let's instantiate the optimizer. \n",
    "#\n",
    "optimizer = optimizer_factory.create_local_optimizer(\n",
    "    optimizer_config=optimizer_config,\n",
    "    optimization_problem=OptimizationProblem(\n",
    "        parameter_space=objective_function.parameter_space,\n",
    "        objective_space=objective_function.output_space,\n",
    "        objectives=[Objective(name='y', minimize=True)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Logger import create_logger\n",
    "from mlos.Optimizers.RegressionModels.RegressionModelFitState import RegressionModelFitState\n",
    "\n",
    "# Let us set up the lists to track optima over time.\n",
    "#\n",
    "best_observation_num_observations = []\n",
    "best_observation_configs = []\n",
    "best_observations = []\n",
    "\n",
    "predicted_value_num_observations = []\n",
    "best_predicted_value_configs = []\n",
    "best_predicted_values = []\n",
    "\n",
    "regression_model_fit_state = RegressionModelFitState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(\"Optimizer evaluation.\")\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "from mlos.Optimizers.RegressionModels.GoodnessOfFitMetrics import GoodnessOfFitMetrics, DataSetType\n",
    "\n",
    "\n",
    "\n",
    "start_iteration_num = i\n",
    "end_iteration_num = start_iteration_num + 100\n",
    "\n",
    "for i in range(start_iteration_num, end_iteration_num):\n",
    "    parameters = optimizer.suggest()\n",
    "    objectives = objective_function.evaluate_point(parameters)\n",
    "    logger.info(f\"[{i+1}/{end_iteration_num}]Parameters: {parameters}, objectives: {objectives}\")\n",
    "    \n",
    "    optimizer.register(parameters.to_dataframe(), objectives.to_dataframe())\n",
    "    \n",
    "    if optimizer.trained:\n",
    "        gof_metrics = optimizer.compute_surrogate_model_goodness_of_fit()\n",
    "        regression_model_fit_state.set_gof_metrics(data_set_type=DataSetType.TRAIN, gof_metrics=gof_metrics)\n",
    "        \n",
    "        \n",
    "    \n",
    "    best_observation_num_observations.append(i)\n",
    "    \n",
    "    best_observation_config, best_observation = optimizer.optimum(OptimumDefinition.BEST_OBSERVATION)    \n",
    "    best_observation_configs.append(best_observation_config)\n",
    "    best_observations.append(best_observation)\n",
    "    \n",
    "    try:\n",
    "        best_predicted_value_config, best_predicted_value = optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "        best_predicted_value_configs.append(best_predicted_value_config)\n",
    "        best_predicted_values.append(best_predicted_value)\n",
    "        predicted_value_num_observations.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best observation dataframe\n",
    "#\n",
    "best_observation_df = pd.DataFrame([observation.to_dict() for observation in best_observations])\n",
    "best_observation_df['num_observations'] = best_observation_num_observations\n",
    "best_observation_df = pd.concat([best_observation_df.drop_duplicates(subset=['y'], keep='last'), best_observation_df.drop_duplicates(subset=['y'], keep='first')]).sort_index()\n",
    "best_observation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predicted_value_df = pd.DataFrame([predicted_value.to_dict() for predicted_value in best_predicted_values])\n",
    "best_predicted_value_df['num_observations'] = predicted_value_num_observations\n",
    "best_predicted_value_df = pd.concat([best_predicted_value_df.drop_duplicates(subset=['predicted_value'], keep='last'), best_predicted_value_df.drop_duplicates(subset=['predicted_value'], keep='first')]).sort_index()\n",
    "best_predicted_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "axs.plot(best_observation_df['num_observations'], best_observation_df['y'], label='y')\n",
    "axs.plot(best_predicted_value_df['num_observations'], best_predicted_value_df['predicted_value'], label='predicted_value')\n",
    "axs.set_ylabel('y')\n",
    "axs.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "axs.set_xticks(best_observation_df['num_observations'][::2])\n",
    "axs.grid(True)\n",
    "axs.set_xlabel('num_observations')\n",
    "axs.legend()  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.RegressionModels.GoodnessOfFitMetrics import DataSetType\n",
    "\n",
    "# Let's take a look at goodness of fit data.\n",
    "#\n",
    "goodness_of_fit_dataframe = regression_model_fit_state.get_goodness_of_fit_dataframe(data_set_type=DataSetType.TRAIN) # TODO: add support to evaluate GoF on test data\n",
    "goodness_of_fit_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "gof_df = goodness_of_fit_dataframe\n",
    "columns_to_plot = [name for name in gof_df.columns.values if name not in ('observation_count', 'prediction_count', 'last_refit_iteration_number')]\n",
    "num_plots = len(columns_to_plot)\n",
    "fig, axs = plt.subplots(num_plots, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "for i, column in enumerate(columns_to_plot):\n",
    "    axs[i].plot(gof_df['last_refit_iteration_number'], gof_df[column], marker='o', label=column)\n",
    "    axs[i].set_ylabel(column)\n",
    "    axs[i].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "    axs[i].set_xticks(gof_df['last_refit_iteration_number'])\n",
    "    axs[i].grid(True)\n",
    "    if i == num_plots - 1:\n",
    "        axs[i].set_xlabel('last_refit_iteration_number')\n",
    "        \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_values.tracer.dump_trace_to_file(r\"E:\\code\\new_mlos\\source\\Mlos.Python\\temp\\optimizer_evaluation_tools.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Bayesian Optimizer Evaluation\n",
    "\n",
    "1. Perform optimizer convergence evaluation on some functions out of the factory:\n",
    "    1. Define some named configurations for the BayesianOptimizer (surrogate model config, utility function, utility function optimizer config).\n",
    "    2. Write the optimum object to:\n",
    "        1. Include all observations (at first - once we know what we don't need, we can remove them)\n",
    "        2. Include surrogate model predictions for all observations.\n",
    "        3. Include a dataframe with the various definitions of optimum:\n",
    "            1. Best observed observation.\n",
    "            2. Observation with the highest:\n",
    "                1. predicted mean\n",
    "                2. upper confidence bound\n",
    "                3. lower confidence bound\n",
    "            3. Speculative optima - kick of the utility function optimizer to find configurations with:\n",
    "                1. maximum predicted mean\n",
    "                2. maximum upper confidence bound\n",
    "                3. maximum lower confidence bound\n",
    "        1. Include a dataframe with cummax of all of the 7 optima.\n",
    "        2. Compare the number of iterations needed to reach the same optimum for the various optimizers.\n",
    "        \n",
    "    3. Train the various models on the various functions and plot all 7 optima as a function of a number of observations (repeat each experiment a few times to get the idea of stability).\n",
    "    4. Progressively increase the difficulty of the optimization problems:\n",
    "        1. Amount of noise.\n",
    "        2. Number of dimensions.\n",
    "        3. Discontinuous functions.\n",
    "        4. Search spaces with more branching and nesting.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Utility Function Optimizer Evaluation\n",
    "\n",
    "This is a two-step process.\n",
    "1. Evaluate these model-free optimizers on the ObjectiveFunctionBase subclasses. This should allow us to capture any bugs, and get a sense of how quickly these optimizers converge on a variety of functions.\n",
    "2. Evaluate these model-free optimizers as part of the bayesian optimizer. The key difference is that the model gets refit, so the underlying function changes between optimizer invocations, but we get to use prior invocations as starting points.\n",
    "\n",
    "3. Plot all of this :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
